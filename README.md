# Llama-hackathon

Setup docker for ollama
get ready training data
RAG

Technologies: 
- ollama
- llama3
- llama3.2:8b
- python
- langchain
- streamlit


FOR TERMINALS:
- 1: ollama serve


- 2: [ANACONDA PROMPT] conda activate ollamavenv | cd C:\Users\Igodo\OneDrive\Desktop\Code\Llama-hackathon\backend | uvicorn ollama-test-fast:app --host 0.0.0.0 --port 8000
- This creates a fastAPI web socket server

- 3: cd C:\Users\Igodo\OneDrive\Desktop\Code\Llama-hackathon\llama-chat   | npm run dev
- This creates a react frontend and runs it on localhost:3000 (most of the time)
